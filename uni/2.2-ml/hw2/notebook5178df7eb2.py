# -*- coding: utf-8 -*-
"""notebook5178df7eb2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QoOPIy5U0EnaUhl0n7nh1yjAwdXqzXZx
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

from keras import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from keras.metrics import Accuracy, Recall, Precision
import keras.backend as K
from sklearn.preprocessing import OneHotEncoder

# import tensorflow as tf
# from tensorflow.keras.metrics import F1Score

# ASSETS_DIR = "assets"
RANDOM_STATE = 123
VERBOSITY = 1


def asdf(link: str, withy=False):
    df = pd.read_csv(link, index_col=0).sort_index()

    if withy:
        y = df.pop("user_id")
        y = y.apply(lambda x: 1 if x == 0 else 0)
        y = pd.DataFrame(y)

    # TODO
    cols_to_drop = ["time", "date", "sites"]
    df = df.drop(columns=cols_to_drop)
    X = df

    if withy:
        return X, y
    return X


X1, y1 = asdf("https://raw.githubusercontent.com/tsepanx/scripts_collection/master/uni/2.2-ml/hw2/task%201.csv", True)
X2 = asdf("task_1_verify.csv")


def onehot_encode(X: pd.DataFrame) -> pd.DataFrame:
    encoder = OneHotEncoder()
    encoder.fit(X)
    np_array = encoder.transform(X).toarray()
    feat_columns = encoder.get_feature_names_out(X.columns)

    res_df = pd.DataFrame(np_array, columns=feat_columns)
    # res_df = res_df.astype(np.int8)
    return res_df


X1 = onehot_encode(X1)
X2 = onehot_encode(X2)

print("Encoding...")

# X = pd.get_dummies(X)
X = onehot_encode(X)

print("Completed encoding")

"""# New Section"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)


def build_model(neurons_cnt=16, layers_cnt=1, dropout_prob=0.5):
    model = Sequential()

    model.add(Dense(neurons_cnt, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dropout(dropout_prob))

    for i in range(layers_cnt):
        model.add(Dense(neurons_cnt, activation='relu'))
        model.add(Dropout(dropout_prob))

    model.add(Dense(1, activation='sigmoid'))

    model.compile(
        optimizer="adam",
        loss="binary_crossentropy",
        metrics=[
            my_f1_score,
            'accuracy'
            # Precision(),
            # Recall(),
            # Accuracy(),
        ]
    )

    return model


def my_f1_score(y_true, y_pred):
    # precision = K.cast(Precision().update_state(y_true, y_pred), 'float32').result().numpy()
    # recall = K.cast(Recall().update_state(y_true, y_pred), 'float32').result().numpy()
    #
    # return 2*((precision*recall)/(precision+recall+K.epsilon()))

    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())
    return f1_val


model = build_model()

# early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
# model.fit(X_train, y_train, validation_split=0.2, epochs=50, callbacks=[early_stopping])
# loss, accuracy = model.evaluate(X_test, y_test)

VALIDATION_RATIO = 0.2
BATCH_SIZE = 128
EPOCHS = 20

early_stopping = EarlyStopping(
    patience=5,
    restore_best_weights=True
)

print("model.fit")
model.fit(
    X_train, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=VALIDATION_RATIO,
    verbose=VERBOSITY,
    callbacks=[early_stopping]
)

# loss, accuracy\
# results = model.evaluate(
#     X_test, y_test,
#     verbose=VERBOSITY
# )
# print(results)

# test_loss, test_f1, test_accuracy, test_precision, test_recall, test_auc = model.evaluate(X_test, y_test)

from sklearn.metrics import classification_report

# evaluate the model without fine tuning
y_pred = model.predict(X_test, verbose=VERBOSITY)
y_pred = y_pred > 0.5
print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier

model = KerasClassifier(build_fn=build_model, verbose=VERBOSITY)

param_grid = {
    'neurons_cnt': [16, 32, 64, 128],
    'layers_cnt': [1, 2, 3],
    'dropout_prob': [0.0, 0.1, 0.2, 0.3, 0.5],
}

# Fine tune model
grid_search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring='f1', verbose=VERBOSITY)
grid_result = grid_search.fit(X_train, y_train, verbose=2)

print("Best f1 score: {:.2f} using {}".format(grid_result.best_score_, grid_result.best_params_))
